{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6378dcd6-9bb7-48d1-a095-7727a24b7173",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- Follow the installation instructions in the readme file\n",
    "- Answer the questions in this notebook\n",
    "- Once your work is finished: restart the kernel, run all cells in order and check that the outputs are correct.\n",
    "- Send your completed notebook to `remy.degenne@inria.fr` with email title `SL_TP4_NAME1_NAME2` (or `SL_TP4_NAME` if you work alone).\n",
    "\n",
    "**Deadline: January 23, 15:00 CET**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aef0b14a-6e93-4f0b-be94-84d090c5d184",
   "metadata": {},
   "source": [
    "# Fitted Q Iteration (FQI)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you will implement the Fitted Q Iteration algorithm (FQI) to solve the [CartPole](https://gymnasium.farama.org/environments/classic_control/cart_pole/) problem.\n",
    "\n",
    "This notebooks will first cover the basics for using the Gymnasium library: how to instantiate an environment, step into it and collect training data from the FQI algorithm.\n",
    "\n",
    "You will then learn how to implement step-by-step the FQI algorithm which is the predecessor of the [Deep Q-Network (DQN)](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008852aa-cf8f-4c81-afaf-7d3be75f389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import os\n",
    "\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2369b55-266c-4dbe-b14d-250ef7386407",
   "metadata": {},
   "source": [
    "## First steps with the Gym interface\n",
    "\n",
    "An environment that follows the [gym interface](https://gymnasium.farama.org/) is quite simple to use.\n",
    "It provides to this user mainly three methods, which have the following signature (for gym versions > 0.26):\n",
    "\n",
    "- `reset()` called at the beginning of an episode, it returns an observation and a dictionary with additional info (defaults to an empty dict)\n",
    "- `step(action)` called to take an action with the environment, it returns the next observation, the immediate reward, whether new state is a terminal state (episode is finished), whether the max number of timesteps is reached (episode is artificially finished), and additional information\n",
    "- (Optional) `render()` which allow to visualize the agent in action. Note that graphical interface does not work on google colab, so we cannot use it directly (we have to rely on `render_mode='rbg_array'` to retrieve an image of the scene).\n",
    "\n",
    "Under the hood, it also contains two useful properties:\n",
    "- `observation_space` which is one of the gym spaces (`Discrete`, `Box`, ...) and describe the type and shape of the observation\n",
    "- `action_space` which is also a gym space object that describes the action space, so the type of action that can be taken\n",
    "\n",
    "The best way to learn about [gym spaces](https://gymnasium.farama.org/api/spaces/) is to look at the [source code](https://github.com/Farama-Foundation/Gymnasium/tree/main/gymnasium/spaces), but you need to know at least the main ones:\n",
    "- `gym.spaces.Box`: A (possibly unbounded) box in $R^n$. Specifically, a Box represents the Cartesian product of n closed intervals. Each interval has the form of one of $[a, b]$, $(-\\infty, b]$, $[a, \\infty)$, or $(-\\infty, \\infty)$. Example: A 1D-Vector or an image observation can be described with the Box space.\n",
    "```python\n",
    "# Example for using image as input:\n",
    "observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)\n",
    "```                                       \n",
    "\n",
    "- `gym.spaces.Discrete`: A discrete space in $\\{ 0, 1, \\dots, n-1 \\}$\n",
    "  Example: if you have two actions (\"left\" and \"right\") you can represent your action space using `Discrete(2)`, the first action will be 0 and the second 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af049724-3db9-4dec-a40c-3aa025735f00",
   "metadata": {},
   "source": [
    "## CartPole Environment\n",
    "\n",
    "For this example, we will use CartPole environment, a classic control problem.\n",
    "\n",
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
    "\n",
    "Cartpole environment: [https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
    "\n",
    "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ec422f-2edd-40df-8da2-c1093e1da38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476f88b6-4fe2-45ec-bddd-4d83feb3a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box(4,) means that it is a Vector with 4 components\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Shape:\", env.observation_space.shape)\n",
    "\n",
    "# Discrete(2) means that there is two discrete actions\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414990e-2cbd-4f6f-b268-42f16a9b253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reset method is called at the beginning of an episode\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a95c8-d64a-43a3-adee-344891bbd1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a random action\n",
    "action = env.action_space.sample()\n",
    "print(f\"Sampled action: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311777a2-9518-496a-b1e8-0eb1af81fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step in the environment\n",
    "obs, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102a9c8-8407-4f88-b58d-4e3e5a00af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note the obs is a numpy array\n",
    "# info is an empty dict for now but can contain any debugging info\n",
    "# reward is a scalar\n",
    "print(obs.shape, reward, terminated, truncated, info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90fbec50-1da0-4bc4-8b88-79fa38480bfe",
   "metadata": {},
   "source": [
    "### Exercise: write the function to collect data\n",
    "\n",
    "This function collects a dataset of transitions that will be used to train a model using the FQI algorithm.\n",
    "\n",
    "See docstring of the function for what is expected as input/output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1b3ff1-911b-4582-91f7-49420380eda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OfflineData:\n",
    "    \"\"\"\n",
    "    A class to store transitions.\n",
    "    \"\"\"\n",
    "\n",
    "    observations: np.ndarray  # same as \"state\" in the theory\n",
    "    next_observations: np.ndarray\n",
    "    actions: np.ndarray\n",
    "    rewards: np.ndarray\n",
    "    terminateds: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f753ed-5f5d-4133-ab82-c84b3cfe2eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env: gym.Env, n_steps: int = 50_000) -> OfflineData:\n",
    "    \"\"\"\n",
    "    Collect transitions using a random agent (sample action randomly).\n",
    "\n",
    "    :param env: The environment.\n",
    "    :param n_steps: Number of steps to perform in the env.\n",
    "    :return: The collected transitions.\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(env.observation_space, spaces.Box)\n",
    "    # Numpy arrays (buffers) to collect the data\n",
    "    observations = np.zeros((n_steps, *env.observation_space.shape))\n",
    "    next_observations = np.zeros((n_steps, *env.observation_space.shape))\n",
    "    # Discrete actions\n",
    "    actions = np.zeros((n_steps, 1))\n",
    "    rewards = np.zeros((n_steps,))\n",
    "    terminateds = np.zeros((n_steps,))\n",
    "\n",
    "    # Variable to know if the episode is over (done = terminated or truncated)\n",
    "    done = False\n",
    "    # Start the first episode\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # You need to collect transitions for `n_steps` using\n",
    "    # a random agent (sample action uniformly).\n",
    "    # Do not forget to reset the environment if the current episode is over\n",
    "    # (done = terminated or truncated)\n",
    "    #\n",
    "    # TODO:\n",
    "    # 1. Sample a random action\n",
    "    # 2. Step in the env using this random action\n",
    "    # 3. Retrieve the new transition data (observation, reward, ...)\n",
    "    #  and update the numpy arrays (buffers)\n",
    "    # 4. Repeat until you collected `n_steps` transitions\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    return OfflineData(\n",
    "        observations,\n",
    "        next_observations,\n",
    "        actions,\n",
    "        rewards,\n",
    "        terminateds,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbe5d4b3-d3d6-4f82-9337-a8566c1fc707",
   "metadata": {},
   "source": [
    "Let's try the collect data method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499a940-a35f-4b8c-86bb-94231c41a87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "env = gym.make(env_id)\n",
    "n_steps = 10_000\n",
    "# Collect transitions for n_steps\n",
    "data = collect_data(env=env, n_steps=n_steps)\n",
    "\n",
    "# Check the length of the collected data\n",
    "assert len(data.observations) == n_steps\n",
    "assert len(data.actions) == n_steps\n",
    "# Check that there are multiple episodes in the data\n",
    "assert not np.all(data.terminateds)\n",
    "assert np.any(data.terminateds)\n",
    "# Check the shape of the collected data\n",
    "if env_id == \"CartPole-v1\":\n",
    "    assert data.observations.shape == (n_steps, 4)\n",
    "    assert data.next_observations.shape == (n_steps, 4)\n",
    "assert data.actions.shape == (n_steps, 1)\n",
    "assert data.rewards.shape == (n_steps,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "239ad0de-bec9-4918-a691-4322d062c45a",
   "metadata": {},
   "source": [
    "## Fitted Q Iteration (FQI) Agent\n",
    "\n",
    "See Lecture 4, slide 31 (and next slides for more explanations in the linear case, although this practical session is not linear).\n",
    "\n",
    "At each iteration of the algorithm, a dataset of transitions is gathered. Then target Q values for each transition are computed and the algorithm solves a regression problem with the transitions as inputs and the target values as outputs to update its Q-value approximation.\n",
    "\n",
    "After the maximal number of iterations is reached, the policy returned is the greedy policy with respect to the current Q-values.\n",
    "\n",
    "### Choosing a regression model\n",
    "\n",
    "With FQI, you can use any regression model to produce a Q-value estimator from a dataset of transitions and targets.\n",
    "\n",
    "Here we are choosing a [k-nearest neighbors regressor](https://scikit-learn.org/stable/modules/neighbors.html#regression), but one could choose a linear model, a decision tree, a neural network, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d873e-bc40-4a35-b98b-4fe9ce916aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First choose the regressor\n",
    "model_class = partial(\n",
    "    KNeighborsRegressor, n_neighbors=30\n",
    ")  # LinearRegression, GradientBoostingRegressor..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bf9544c-87f4-4872-9ae9-eef49c3a040a",
   "metadata": {},
   "source": [
    "### 1. Exercise: write the function to predict Q-Values\n",
    "\n",
    "In FQI, we will need to compute, for any transition $(s, a, r, s')$, a target value $y = r + \\gamma \\cdot \\max_{a' \\in A}(Q^{n-1}_\\theta(s', a'))$. In order to do that, we need to be able to compute the current Q-value of state-action pairs.\n",
    "\n",
    "See docstring of the function for what is expected as input/output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38316a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_values(\n",
    "    model: RegressorMixin,\n",
    "    obs: np.ndarray,\n",
    "    n_actions: int,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Retrieve the q-values for a set of observations obs.\n",
    "    Q(s, action) for all s in obs and all possible actions.\n",
    "\n",
    "    :param model: Q-value estimator\n",
    "    :param obs: A batch of observations\n",
    "    :param n_actions: Number of discrete actions.\n",
    "    :return: The predicted q-values for the given observations\n",
    "        (batch_size, n_actions)\n",
    "    \"\"\"\n",
    "    batch_size = len(obs)\n",
    "    q_values = np.zeros((batch_size, n_actions))\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    # TODO: for every possible actions a:\n",
    "    # 1. Create the regression model input $(s, a)$ for the action a\n",
    "    # and states s (here a batch of observations)\n",
    "    # 2. Predict the q-values for the batch of states (use model.predict)\n",
    "    # 3. Update q-values array for the current action a\n",
    "\n",
    "\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    return q_values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be7a24b4-a416-4607-af3a-9dfdbc203fc1",
   "metadata": {},
   "source": [
    "### Create the Agent\n",
    "To create an agent, rlberry requires to use a **very simple interface**, with basically two methods to implement: `fit()` and `eval()`.\n",
    "\n",
    "You can find more information on this interface [here(AgentWithSimplePolicy)](rlberry.agents.agent.AgentWithSimplePolicy).\n",
    "### function fit() :\n",
    "\n",
    "#### 1. First Iteration\n",
    "\n",
    "For $n = 0$, the initial training set is defined as:\n",
    "\n",
    "- $x = (s_t, a_t)$\n",
    "- $y = r_t$\n",
    "\n",
    "We fit a regression model $f_\\theta(x) = y$ to obtain $ Q^{n=0}_\\theta(s, a) $\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Exercise: the fitted Q iterations\n",
    "\n",
    "1. Create the training set based on the previous iteration $ Q^{n-1}_\\theta(s, a) $ and the transitions:\n",
    "- input: $x = (s_t, a_t)$\n",
    "- if $s_{t+1}$ is non-terminal: $y = r_t + \\gamma \\cdot \\max_{a' \\in A}(Q^{n-1}_\\theta(s_{t+1}, a'))$\n",
    "- if $s_{t+1}$ is terminal, do not bootstrap: $y = r_t$\n",
    "\n",
    "2. Fit a model $f_\\theta$ using a regression algorithm to obtain $ Q^{n}_\\theta(s, a)$\n",
    " \n",
    "\\begin{aligned}\n",
    " f_\\theta(x) = y\n",
    "\\end{aligned}\n",
    "\n",
    "4. Repeat, $n = n + 1$\n",
    "\n",
    "### function evaluate() :\n",
    "\n",
    "#### 3. Exercise: write the function to evaluate a model\n",
    "\n",
    "A greedy policy $\\pi(s)$ can be defined using the q-value:\n",
    "\n",
    "$\\pi(s) = argmax_{a \\in A} Q(s, a)$.\n",
    "\n",
    "It is the policy that take the action with the highest q-value for a given state.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe29bd2-ea95-4778-b466-537a088615b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlberry.agents import Agent\n",
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "\n",
    "class Fitted_Q_Iteration(Agent):\n",
    "    name = \"Fitted_Q_Iteration\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        model_class,\n",
    "        n_steps_collection=50_000,  # Number of steps to perform in the env to collect data.\n",
    "        eval_freq=2,\n",
    "        n_eval_episodes=10,\n",
    "        gamma=0.99,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        # it's important to put **kwargs to ensure compatibility with the base class\n",
    "        # self.env is initialized here\n",
    "        super().__init__(env=env, **kwargs)\n",
    "\n",
    "        self.model_class = model_class  # The Agent's regression model class\n",
    "        self.eval_freq = eval_freq  # How often do we evaluate the learned model\n",
    "        self.n_eval_episodes = n_eval_episodes  # How many episodes to evaluate every eval-freq\n",
    "        \n",
    "        self.gamma = 0.99  # discount factor\n",
    "        self.n_actions = int(self.env.action_space.n)  # Number of discrete actions\n",
    "\n",
    "        # Collect data : transitions for n_steps\n",
    "        self.current_data = collect_data(env=self.env, n_steps=n_steps_collection)\n",
    "\n",
    "        # 1 - First iteration:\n",
    "        # The target q-value is the reward obtained\n",
    "        targets = self.current_data.rewards.copy()\n",
    "        # Create input for current observations and actions\n",
    "        # Concatenate the observations and actions\n",
    "        # so we can predict qf(s_t, a_t)\n",
    "        self.current_obs_input = np.concatenate(\n",
    "            (self.current_data.observations, self.current_data.actions), axis=1\n",
    "        )\n",
    "        # Fit the estimator for the current target\n",
    "        self.current_model = model_class().fit(self.current_obs_input, targets)\n",
    "\n",
    "    def fit(self, budget, **kwargs):\n",
    "        # 2 - the fitted Q iterations\n",
    "        for iter_idx in range(budget):\n",
    "            ### YOUR CODE HERE\n",
    "            # TODO:\n",
    "            # 1. Compute the q values for the next states using\n",
    "            # the previous regression model\n",
    "            # 2. Keep only the next q values that correspond\n",
    "            # to the greedy-policy\n",
    "            # 3. Construct the regression target (TD(0) target)\n",
    "            # 4. Fit a new regression model with this new target\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            ### END OF YOUR CODE\n",
    "\n",
    "            if (iter_idx + 1) % self.eval_freq == 0:\n",
    "                print(f\"Iter {iter_idx + 1}\")\n",
    "                print(\n",
    "                    f\"Score: {self.current_model.score(self.current_obs_input, targets):.2f}\"\n",
    "                )\n",
    "                final_eval_result = self.eval(self.n_eval_episodes)\n",
    "\n",
    "        info = {\"final_eval_result\": final_eval_result}\n",
    "        return info  # return the final eval mean, but more informations are directly displayed by the eval() function in the output terminal.\n",
    "\n",
    "    # 3 - function to evaluate a model\n",
    "    def eval(\n",
    "        self,\n",
    "        n_simulations: int = 10,\n",
    "        video_name: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        episode_returns, episode_reward = [], 0.0\n",
    "        total_episodes = 0\n",
    "        done = False\n",
    "\n",
    "        if not self.eval_env:\n",
    "            self.eval_env = self.env\n",
    "\n",
    "        # Setup video recorder\n",
    "        video_recorder = None\n",
    "        if video_name is not None and self.eval_env.render_mode == \"rgb_array\":\n",
    "            os.makedirs(\"./logs/videos/\", exist_ok=True)\n",
    "\n",
    "            video_recorder = VideoRecorder(\n",
    "                env=self.eval_env,\n",
    "                base_path=f\"./logs/videos/{video_name}\",\n",
    "            )\n",
    "\n",
    "        obs, _ = self.eval_env.reset()\n",
    "        n_actions = int(self.eval_env.action_space.n)\n",
    "        assert isinstance(\n",
    "            self.eval_env.action_space, spaces.Discrete\n",
    "        ), \"FQI only support discrete actions\"\n",
    "\n",
    "        while total_episodes < n_simulations:\n",
    "            # Record video\n",
    "            if video_recorder is not None:\n",
    "                video_recorder.capture_frame()\n",
    "\n",
    "            ### YOUR CODE HERE\n",
    "\n",
    "            # Retrieve the q-values for the current observation\n",
    "            # you need to re-use `get_q_values()`\n",
    "            # Then select the action that maximizes the q-value for each state\n",
    "            # Do a step in the env using the selected action\n",
    "\n",
    "            \n",
    "\n",
    "            ### END OF YOUR CODE\n",
    "\n",
    "            episode_reward += float(reward)\n",
    "\n",
    "            done = terminated or truncated\n",
    "            if done:\n",
    "                episode_returns.append(episode_reward)\n",
    "                episode_reward = 0.0\n",
    "                total_episodes += 1\n",
    "                obs, _ = self.eval_env.reset()\n",
    "\n",
    "        if video_recorder is not None:\n",
    "            print(f\"Saving video to {video_recorder.path}\")\n",
    "            video_recorder.close()\n",
    "\n",
    "        print(\n",
    "            f\"Total reward = {np.mean(episode_returns):.2f} +/- {np.std(episode_returns):.2f}\"\n",
    "        )\n",
    "\n",
    "        return np.mean(episode_returns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d824b98-6360-4d4e-a3e2-584c0c36665e",
   "metadata": {},
   "source": [
    "### Performing experiments\n",
    "\n",
    "First, let's define some constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16a4e16-e2a8-432f-885f-27fbe0f61f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlberry.envs import gym_make\n",
    "from rlberry.manager import ExperimentManager\n",
    "\n",
    "# Max number of iterations\n",
    "n_iterations = 20\n",
    "# How often do we evaluate the learned model\n",
    "eval_freq = 2\n",
    "# How many episodes to evaluate every eval-freq\n",
    "n_eval_episodes = 10\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "# Number of discrete actions\n",
    "n_actions = int(env.action_space.n)\n",
    "\n",
    "\n",
    "env_id = \"CartPole-v1\"  # Id of the environment\n",
    "env_ctor = gym_make  # constructor for the env\n",
    "env_kwargs = dict(id=env_id)  # give the id of the env inside the kwargs\n",
    "\n",
    "eval_env_kwargs = dict(id=env_id, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeaf39d-0a48-4c68-aa5b-7b734e7bcc7f",
   "metadata": {},
   "source": [
    "Now let's create an `ExperimentManager`, which is a class used to run experiments with specified agents and environments.\n",
    "\n",
    "The experiment manager spawns agents and environments for training and then once the agents are trained, it uses these agents and new environments to evaluate how well the agents perform. All of these steps can be done several times to assess stochasticity of agents and/or environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbff0d8-eab0-4cec-847e-23bc1c509359",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_experiment = ExperimentManager(\n",
    "    Fitted_Q_Iteration,  # Agent Class\n",
    "    (env_ctor, env_kwargs),  # Environment as Tuple(constructor,kwargs)\n",
    "    init_kwargs=dict(\n",
    "        model_class=model_class,\n",
    "        n_steps_collection=50_000,\n",
    "        eval_freq=eval_freq,\n",
    "        n_eval_episodes=n_eval_episodes,\n",
    "        gamma=gamma,\n",
    "    ),\n",
    "    eval_env=(env_ctor, eval_env_kwargs),\n",
    "    fit_budget=int(n_iterations),  # Budget used to call our agent \"fit()\"\n",
    "    n_fit=1,  # Number of agent instances to fit.\n",
    "    agent_name=\"Agent_FQI_\" + env_id,  # Name of the agent\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f248b-14d3-468e-a218-d1164740006c",
   "metadata": {},
   "source": [
    "Use `fit()` to train an agent and then `eval_agents()` to evaluate the trained agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a49dc-8187-4b13-9dcc-cd49148673ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_experiment.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3477740-bfe4-4e0b-8a5d-cd0206786c8b",
   "metadata": {},
   "source": [
    "### Record a video of the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d90cc-2d77-4682-a97c-a3dacd081f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = f\"FQI_{env_id}\"\n",
    "n_eval_episodes = 3\n",
    "my_experiment.eval_agents(\n",
    "    eval_kwargs=dict(n_simulations=n_eval_episodes, video_name=video_name)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "\n",
    "def show_videos(video_path: str = \"\", prefix: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "    :param video_path: Path to the folder containing videos\n",
    "    :param prefix: Filter the video, showing only the only starting with this prefix\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    for mp4 in Path(video_path).glob(f\"{prefix}*.mp4\"):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append(\n",
    "            \"\"\"<video alt=\"{}\" autoplay\n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                </video>\"\"\".format(\n",
    "                mp4, video_b64.decode(\"ascii\")\n",
    "            )\n",
    "        )\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc66910-de41-4622-9811-96addf8ba8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"FQI agent on {env_id} after {n_iterations} iterations:\")\n",
    "show_videos(\"./logs/videos/\", prefix=video_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69a60230-6477-4318-8d60-10f6eada6064",
   "metadata": {},
   "source": [
    "### Going further (optional)\n",
    "\n",
    "- play with different models, and with their hyperparameters\n",
    "- play with the discount factor\n",
    "- play with the number of data collected/used\n",
    "- combine data from random policy with data from trained model\n",
    "- Use a neural network as the regression model (Scikit-learn has a class for simple fully connected neural networks)\n",
    "- Implement DQN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40b8a44f-d48c-4a9d-8ed4-968c1ffb9948",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "What we have seen in this notebook:\n",
    "- collecting data using a random agent in a gym environment\n",
    "- predicting q-values using a regression model\n",
    "- the fitted q-iteration (FQI) algorithm to learn from an offline dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761010b8-86dd-4958-b097-2912606b4493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
