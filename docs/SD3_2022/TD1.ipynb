{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830b05b9",
   "metadata": {},
   "source": [
    "# Classification: k plus proches voisins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d312f348",
   "metadata": {},
   "source": [
    "Documentation des librairies utilisées dans ce tp:\n",
    "- scikit-learn: [https://scikit-learn.org/stable/index.html](https://scikit-learn.org/stable/index.html)\n",
    "- matplotlib: [https://matplotlib.org/](https://matplotlib.org/)\n",
    "- numpy: [https://numpy.org/](https://numpy.org/)\n",
    "\n",
    "Instructions d'installation dans un environnement conda, si vous voulez utiliser votre propre ordinateur (inutile de suivre ces instructions sur les machines de la salle info).\n",
    "\n",
    "- Créer un nouvel environnement, appelé `sd3` (vous avez besoin d'avoir anaconda ou miniconda installé)\n",
    "```\n",
    "conda create -n sd3 python=3.9\n",
    "```\n",
    "- Activer l'environnement\n",
    "```\n",
    "conda activate sd3\n",
    "```\n",
    "- Installer les librairies requises\n",
    "```\n",
    "conda install -c conda-forge jupyterlab matplotlib scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe quelques librairies utiles\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27f411",
   "metadata": {},
   "source": [
    "## Données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45e8cae",
   "metadata": {},
   "source": [
    "On va commencer par rassembler nos données. On utilisera dans ce TP le dataset (ensemble de\n",
    "données) Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefa9463",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24246a29",
   "metadata": {},
   "source": [
    "On peut maintenant écrire `print(iris)` pour voir ce que contient l’objet iris qu’on vient de définir. C’est un dictionnaire. On peut obtenir la liste des champs de ce dictionnaire en tapant `print(iris.keys())`. Ceux qui nous intéressent sont data, target et target names. data contient les données et target contient leur classe.\n",
    "\n",
    "- Combien de données sont présentes dans iris ?\n",
    "- Combien d’attributs ont ces données ?\n",
    "- Combien de classes (ou étiquettes) différentes ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29ebc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f4e9719",
   "metadata": {},
   "source": [
    "Pour ce TP, on ne va utiliser que les deux premiers attributs des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb2f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, :2]\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dbede8",
   "metadata": {},
   "source": [
    "X contient maintenant nos données, qui ont chacune deux attributs. On se restreint à deux attributs\n",
    "pour pouvoir faire des graphiques plus facilement. Mais idéalement, on voudra ensuite utiliser tous\n",
    "les attributs disponibles. Le couple (X, Y) est un jeu d'exemples, comme l'ensemble $\\mathcal X$ du cours. \n",
    "\n",
    "On peut visualiser les données et leurs classes sur une figure de type `scatter` (ensemble de points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f07a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cree une \"figure\", qui est le cadre dans lequel on mettra le graphique\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# L'objet col contient 'k' 'r' ou 'b' en fonction de la classe du point, qui sont des codes couleurs\n",
    "col = np.where(iris.target_names[Y]=='setosa','r',\n",
    "    np.where(iris.target_names[Y]=='versicolor','b','k'))\n",
    "\n",
    "# On affiche un graphique \"scatter\", c'est a dire avec un point pour chaque donnee\n",
    "plt.scatter(x=X[:, 0], y=X[:, 1], c=col)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b42ec",
   "metadata": {},
   "source": [
    "On a obtenu une représentation graphique de nos données, où les coordonnées des points sont\n",
    "les attributs et la couleur représente l’étiquette."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1eb436",
   "metadata": {},
   "source": [
    "Nous ne sommes pas intéressés par la performance d’un classeur sur les données qui ont servi à le construire. On veut un classeur qui généralise, c’est à dire un classeur qui permet de bien classer des données qu’il n’a encore jamais vues.\n",
    "\n",
    "Pour construire puis évaluer un classeur, on sépare les données en deux groupes: les données d’apprentissage et les données de test. Les données d’apprentissage sont utilisées pour construire le classeur et les données de test sont utilisées pour l’évaluer.\n",
    "\n",
    "On va mélanger les données puis les découper en deux ensembles:\n",
    "- Utiliser la fonction `shuffle` de `scikit-learn` pour mélanger X et Y. shuffle permet de mélanger plusieurs tableaux \"en même temps\", c'est à dire que la même permutation aléatoire est utilisée pour les deux tableaux.\n",
    "- Couper X et Y en deux ensembles d'exemples: `X_train, Y_train` qu'on utilisera pour construire le classeur, et `X_test, Y_test` qu'on utilisera pour estimer son erreur de généralisation. Placer environ 3/4 des données dans l'ensemble d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98833fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9d86c7d",
   "metadata": {},
   "source": [
    "## Classeur: k plus proches voisins (k-NN)\n",
    "\n",
    "Le but du tp est d’expérimenter avec le classeur des k plus proches voisins (k-NN) et de l’évaluer.\n",
    "\n",
    "Le principe du classeur k-NN est le suivant:\n",
    "- Construction: stocker tous les exemples.\n",
    "- Evaluation: étant donné une nouvelle donnée $x$,\n",
    "     - Calculer la distance $d_i$ de $x$ à $x_i$ pour tout $i$ de 1 à $N$ (nombre de données)\n",
    "     - Trouver les $k$ données $x_i$ les plus proches de $x$\n",
    "     - Retourner l’étiquette majoritaire parmi les $k$ plus proches voisins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f33c6",
   "metadata": {},
   "source": [
    "La librairie scikit-learn propose une implémentation de k-NN: la fonction `neighbors.KNeighborsClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "clf = neighbors.KNeighborsClassifier(k, weights='uniform', algorithm='brute')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297a88b",
   "metadata": {},
   "source": [
    "Pour constuire le classeur, on utilise sa fonction `fit`. Cette fonction prend en entrée des données et leurs étiquettes, et construit le classeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d212051b",
   "metadata": {},
   "source": [
    "Pour classer de nouvelles données, on utilise la fonction `predict`, qui prend un tableau de données\n",
    "en entrée et retourne un tableau de classes. Pour une unique donnée `x`, on écrit `clf.predict([x])`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a3c51",
   "metadata": {},
   "source": [
    "Pour évaluer la qualité de la classification obtenue, on peut compter le nombre d’erreurs.\n",
    "\n",
    "- Ecrire une fonction `error_rate(clf, X, Y)` qui prend en entrée un classeur, des données et leurs étiquettes et retourne la proportion d'exemples mal classés. Un exemple est mal classé si la classe que le classeur prédit pour la donnée n'est pas égale à sa vraie classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ad471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(clf, X, Y):\n",
    "    # TODO\n",
    "    return 0\n",
    "\n",
    "print(error_rate(clf, X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6241f65",
   "metadata": {},
   "source": [
    "- Pour le paramètre `k = 5`, construire un classeur k-NN à partir de `X_train, Y_train`. Calculer son taux d'erreur sur les ensembles d'entrainement et de test.\n",
    "- Afficher les données sur un plot `scatter`, en bleu si elles sont bien classées, en rouge sinon\n",
    "\n",
    "Attention: on n'utilise jamais les données de test pour construire/entrainer un classeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10aa082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a64b7cc7",
   "metadata": {},
   "source": [
    "- Quel est l'ensemble des valeurs possibles pour `k`? En particulier, quelle est la valeur maximale au delà de laquelle accroitre `k` ne change plus le classeur?\n",
    "- Faire varier le paramètre k du classeur et, pour chaque valeur, entrainer le classeur sur les exemples d'entrainement et calculer la proportion d’erreurs sur l'ensemble d'entrainement et sur l'ensemble de test. Afficher sur un même graphique les deux courbes d'erreurs obtenues.\n",
    "     - Pour quelle valeur de k on a le moins d’erreurs sur les données d’apprentissage ? et sur les données de test ? Expliquer la différence.\n",
    "     - Que se passe-t-il quand k est maximal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419f4634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7edad1df",
   "metadata": {},
   "source": [
    "- L’argument weights de `neighbors.KNeighborsClassifier` permet de donner un poids différent aux données en fonction de leur distance à la nouvelle donnée pour laquelle on veut faire une prédiction. Essayez `weights=’distance’` pour utiliser un poids qui décroit avec la distance.\n",
    "- Chercher dans la documentation de `scikit-learn` comment les poids des données sont calculés quand on utilise `weights=’distance’`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc6fda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a198586",
   "metadata": {},
   "source": [
    "## Pour aller plus loin\n",
    "\n",
    "- Utilisez tous les attributs au lieu d’en utiliser deux. Est-ce que le classeur obtenu est plus performant ?\n",
    "\n",
    "- Implémentez k-NN vous même."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9f38c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
