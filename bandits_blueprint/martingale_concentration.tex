% !TeX root = blueprint.tex

The analysis of algorithms in stochastic sequential learning problems relies on the identification of ``typical'' events, stating that the observations generated by the interaction are not ``too unlikely''. Then if an algorithm behaves well on those typical events, it behaves well in expectation.

We need to quantify the deviation of the empirical observations from their means or true distribution.

\subsection{Sub-Gaussian random variables}
\label{sub:sub_gaussian_random_variables}

Warning: we need \textbf{conditionally} sub-Gaussian random variables. Most papers don't note the difference with (unconditionally) sub-Gaussian, but there is one.

\begin{definition}
A random variable $X : \Omega \to \mathbb{R}$ is said to be $\sigma^2$-sub-Gaussian conditionally to a $\sigma$-algebra $\mathcal G$ if for all $\lambda \in \mathbb{R}$, almost surely
\begin{align*}
\mathbb{E}[e^{\lambda X} \mid \mathcal G] \le e^{\frac{1}{2}\sigma^2\lambda^2} \: .
\end{align*}

A random variable $X : \Omega \to \mathbb{R}$ is said to be $\sigma^2$-sub-Gaussian if for all $\lambda \in \mathbb{R}$,
\begin{align*}
\mathbb{E}[e^{\lambda X}] \le e^{\frac{1}{2}\sigma^2\lambda^2} \: .
\end{align*}
\end{definition}

Can we generalize these two definitions in the same way we generalized independence?

\begin{definition}
A random variable $X : \Omega \to \mathbb{R}$ is said to be $\sigma^2$-sub-Gaussian with respect to a map $\kappa : \Omega' \to \mathcal M(\Omega)$ and a measure $\mu' : \Omega'$ if for all $\lambda \in \mathbb{R}$, $\mu'$-almost surely
\begin{align*}
\kappa(\omega')[e^{\lambda X}] \le e^{\frac{1}{2}\sigma^2\lambda^2} \: .
\end{align*}
\end{definition}

Does that definition help? TODO: check that the expectation with respect to the measure associated to the conditional expectation is the conditional expectation: $(s \mapsto \mathbb{E}[\mathbb{1}_s \mid \mathcal G](\omega'))[f] = \mathbb{E}[f \mid \mathcal G](\omega')$.

\subsection{Azuma-Hoeffding inequality}
\label{sub:azuma_hoeffding_inequality}

\begin{theorem}
Let $(X_i)_{i\in \mathbb{N}}$ be a martingale difference sequence with natural filtration $(\mathcal F_i)_{i\in \mathbb{N}}$ such that $X_i$ is $\sigma^2$-sub-Gaussian conditionally on $\mathcal F_{i-1}$. Let $n \in \mathbb{N}$ and $\varepsilon > 0$. Then
\begin{align*}
\mathbb{P}(\sum_{i=1}^n X_i \ge \varepsilon) \le \exp \left( - \frac{\varepsilon^2}{2 \sum_{i=1}^n \sigma_i^2} \right) \: .
\end{align*}
\end{theorem}

TODO: with the def of sub-Gaussian we have above, which is centered at 0, the sub-Gaussian condition on the $X_i$ implies the martingale difference sequence hypothesis.

\begin{proof}
Let $\lambda \ge 0$. Since $x \mapsto e^{\lambda x}$, $\mathbb{P}(\sum_{i=1}^n X_i \ge \varepsilon) = \mathbb{P}(e^{\lambda\sum_{i=1}^n X_i} \ge e^{\lambda\varepsilon})$. We then use Chernov's inequality to get
\begin{align*}
\mathbb{P}(\sum_{i=1}^n X_i \ge \varepsilon)
\le e^{-\lambda \varepsilon} \mathbb{E}\left[ e^{\lambda\sum_{i=1}^n X_i} \right]
\: .
\end{align*}

We use the tower rule to introduce conditioning on the $\sigma$-algebras $\mathcal F_i$.
\begin{align*}
\mathbb{E}\left[ e^{\lambda\sum_{i=1}^n X_i} \right]
= \mathbb{E}\left[ \mathbb{E}\left[ e^{\lambda\sum_{i=1}^n X_i} \mid \mathcal F_{n-1} \right] \right]
&= \mathbb{E}\left[ \mathbb{E}\left[ e^{\lambda X_n} \mid \mathcal F_{n-1} \right] e^{\lambda\sum_{i=1}^n X_i} \right]
\\
&= \mathbb{E}\left[\prod_{i=1}^n \mathbb{E}\left[ e^{\lambda X_i} \mid \mathcal F_{i-1} \right] \right]
\: .
\end{align*}
Formally, the last equality requires a proof by induction.

We now use the sub-Gaussian hypothesis.
\begin{align*}
\mathbb{E}\left[ e^{\lambda\sum_{i=1}^n X_i} \right]
&= \mathbb{E}\left[\prod_{i=1}^n \mathbb{E}\left[ e^{\lambda X_i} \mid \mathcal F_{i-1} \right] \right]
\le \mathbb{E}\left[\prod_{i=1}^n e^{\frac{1}{2} \sigma_i^2 \lambda^2} \right]
= e^{\frac{1}{2} (\sum_{i=1}^n \sigma_i^2) \lambda^2}
\: .
\end{align*}

We finally optimize over $\lambda \ge 0$ to get the result.
\end{proof}
