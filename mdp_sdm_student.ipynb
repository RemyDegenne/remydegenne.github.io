{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Markov Decision Processes (MDP) $\\langle S, A, R, P, P_0, H, \\gamma\\rangle$\n",
        "\n",
        "$S$ a set of states.\\\n",
        "$A$ a set of actions.\\\n",
        "$R$ a reward mapping : $S \\times A \\times S \\rightarrow \\mathbb{R}$.\\\n",
        "$P$ the transition probabibility distribution $S \\times A \\rightarrow \\Delta S$.\\\n",
        "$P_0$ the initial distribution over states: $s_0 \\in S \\sim P_0$.\\\n",
        "$H$ the horizon: the number of transition per episode.\\\n",
        "$\\gamma$ the discount factor in $(0,1)$.\n"
      ],
      "metadata": {
        "id": "2Q68OSoASm9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MDPs are used to model sequential decision tasks in which at each time step $t$, an agent has to take an action given its current state. From $t=0$ to $t=H$, an agent observes state $s_t$ and takes action $a_t$. The agent then transitions to $s_{t+1}$ with probability $P(s_{t+1}|s_t, a_t)$ and receives reward $r_t = R(s_t,a_t,s_{t+1})$. At $t=0$, the agent \"spawns\" in an initial state $s_0$ with probability $P_0(s_0)$. One can also define terminal states that are such that an agent will not take actions in those states: in such terminal states the MDP stops even though $t<H$. An episode or a trajectory in a MDP, is a sequence of length at most $H$ of transition tuples ($s_t, a_t, r_t, s_{t+1}$)."
      ],
      "metadata": {
        "id": "2duHvG0tXLFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Dynamic Programmic or Reinforcement Learning, it is possible to train agents in MDPs in order to learn good solutions for sequential decision tasks. Such solutions are called policies and are mappings from actions to distributions over actions: $\\Pi: S \\rightarrow \\Delta A$. The goal of Dynamic Programming and RL is to find the optimal policy $\\pi^* = \\underset{\\pi \\in \\Pi}{\\operatorname{argmax}} \\underset{s_0 \\sim P_0,a_t \\sim \\pi(s_t),s_{t+1}\\sim P(s_t, a_t)}{\\mathbb{E}}\\left [ \\underset{t=0}{\\overset{H}{\\sum}}\\gamma^t R(s_t, a_t, s_{t+1})\\right ]$. The latter objective function finds policies that maximize the expected cumulative reward of the MDP: we want to find a policy that will, in expectation, generate MDP episodes with high rewards!"
      ],
      "metadata": {
        "id": "xJ3RmlC8d472"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Exercise 1: Implement a Markov Decicion Process class.\n",
        "In the exercises, we will consider that states and actions are finite: $|S| < \\infty, |A| < \\infty $. For such discrete MDPs, the optimal policy is deterministic i.e $\\pi^*: S \\rightarrow A$.\n",
        "\n"
      ],
      "metadata": {
        "id": "kYSbx4kZZa6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "EvZgNWOo-UWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mdp:\n",
        "    \"\"\"\n",
        "    defines a Markov Decision Process\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        states: list,\n",
        "        actions: list,\n",
        "        initial_distribution: np.array, #P0\n",
        "        transition_probability: np.array, #P\n",
        "        reward_function: np.array, #R\n",
        "        gamma: float = 0.9,\n",
        "        terminal_states: list = [],\n",
        "        Horizon: int = 50,\n",
        "    ):\n",
        "        self.states = states\n",
        "        self.nb_states = len(states)\n",
        "        self.terminal_states = terminal_states\n",
        "\n",
        "        self.actions = actions\n",
        "        self.nb_actions = len(actions)\n",
        "\n",
        "        self.P = transition_probability\n",
        "        self.P0 = initial_distribution  # distribution used to draw the first state of the agent, ased in method reset()\n",
        "\n",
        "        self.R = reward_function\n",
        "\n",
        "        self.Horizon = Horizon  # maximum length of an episode\n",
        "        self.gamma = gamma  # discount factor\n",
        "\n",
        "        self.timestep = 0\n",
        "        self.current_state = None\n",
        "\n",
        "        assert self.check_P_is_distrib(), \"the transition matrix is not a distribution over (S,A)\"\n",
        "        assert self.check_P0_is_distrib(), \"initial state is not drawn according to a distribution\"\n",
        "\n",
        "\n",
        "    # TODO: write methods to check arrays are probability distributions\n",
        "    def check_P_is_distrib(self):\n",
        "        pass\n",
        "    def check_P0_is_distrib(self):\n",
        "        pass\n",
        "\n",
        "    def reset(self):  # Initializes an episode and returns the state of the agent.\n",
        "        self.current_state = np.random.choice(a=self.states, p=self.P0)\n",
        "        self.timestep = 0\n",
        "        return self.current_state\n",
        "\n",
        "    def done(self):  # returns True if an episode is over\n",
        "        # TODO: An episode is over if the current state is a terminal state,\n",
        "        # OR if the length of the episode is greater than the Horizon..\n",
        "        pass\n",
        "\n",
        "    def step(self, a):  # Given the current state, and an action, performs a transtion in the MDP,\n",
        "        # TODO: Draw the next state according to the transition matrix.\n",
        "        next_state = ...\n",
        "        # TODO: Get the reward of the transition.\n",
        "        reward = ...\n",
        "\n",
        "        self.timestep += 1\n",
        "        self.current_state = next_state\n",
        "        done = self.done()  # checks if the episode is over\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def one_episode(self, policy):\n",
        "        state = self.reset()\n",
        "        done = False\n",
        "        score = 0\n",
        "        t = 0\n",
        "        while not done:\n",
        "            action = policy(state)\n",
        "            next_state, r, done = self.step(action)\n",
        "            score += r\n",
        "            t += 1\n",
        "            state = next_state\n",
        "        return score\n"
      ],
      "metadata": {
        "id": "IYGbBEW9aar-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Exercise 2: Model the blackjack with dice game using your MDP class and evaluate the performances of simple policies."
      ],
      "metadata": {
        "id": "lWsZsN8LmF_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In blackjack with a die, the goal is to throw as many die as one wants, sequentially, in order to obtain a sum of dice as close as possible to 21. Take your time to model the problem.\n",
        "An episode, i.e a single blackjacke with a die game, looks like this: agent throws a die, check the result, decides to throw again or not, if it threw, checks the sum of dice and repeat, else, the game is over. The score of the agent is the final sum of the dice (or 0, if the sum is strictly greater than 21).\n",
        "- What should the states be ?\n",
        "- What does the probability tranisition matrix looks like ?\n",
        "- Are there terminal transitions ?\n",
        "- What is the reward at time $t$ if the agent score is the *final* sum of dice?"
      ],
      "metadata": {
        "id": "Mipbo3TFnWJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### BLACKJACK WITH DICE ###\n",
        "S = np.arange(22+1) # s = 0 is the initial state before any die is thrown.\n",
        "                    # s = 22 is a terminal state for dice sums > 21.\n",
        "A = [0, 1] # To throw, or not.\n",
        "\n",
        "# The initial state of the MDP is always s = 0.\n",
        "P0 = np.zeros(len(S))\n",
        "P0[0] = 1\n",
        "\n",
        "# The transition probabilities of having a dice sum of 18 know we throw a dice\n",
        "# and we currently have a sum of 16 is the probability of getting a two in a die\n",
        "# throw: P(snext=18|s=16,a=1) = 1/6.\n",
        "# We strore P in a |S|x|A|x|S| matrix.\n",
        "P = np.zeros((len(S), len(A), len(S)))\n",
        "for s in S:\n",
        "    for a in A:\n",
        "        for s_next in S:\n",
        "            P[s, a, s_next] = ... # TODO\n",
        "\n",
        "# Implement the reward function as |S|x|A|x|S| matrix\n",
        "R = np.zeros((len(S), len(A)), len(S))\n",
        "for s in S:\n",
        "    for a in A:\n",
        "        for s_next in S:\n",
        "            R[s, a, s_next] = ... # TODO\n",
        "\n",
        "blackjack_dice = Mdp(S, A, P0, P, R)"
      ],
      "metadata": {
        "id": "wNiltMIwnVWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple class for policies (a policy $\\pi$ takes a state $s$ and returns an action $a$)."
      ],
      "metadata": {
        "id": "CnSPmuENS5Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy:\n",
        "    def __init__(self, mdp:Mdp):\n",
        "        self.mdp = mdp\n",
        "\n",
        "    def get_action(self, state):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Uniform(Policy):\n",
        "    def __init__(self, mdp:Mdp):\n",
        "        super().__init__(mdp)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        return np.random.randint(self.mdp.nb_actions)"
      ],
      "metadata": {
        "id": "MpXVDnFxvJPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a simple policy that throw a die only if the score is below 16."
      ],
      "metadata": {
        "id": "rQYX6vzUaR-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ThresholdPolicy(Policy):\n",
        "    def __init__(sefl, mdp):\n",
        "        super().__init__(mdp)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        # TODO (if you have pbm, hint: turn into an int)\n",
        "        pass"
      ],
      "metadata": {
        "id": "zBiltIc7aZqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pol_unif = Uniform(blackjack_dice).get_action\n",
        "# Let us check the average performance of the random policy on 100 blackjack with dice games.\n",
        "policy_unif_eval = []\n",
        "for episodes in range(100):\n",
        "    policy_unif_eval.append(blackjack_dice.one_episode(pol_unif))\n",
        "\n",
        "pol_thresh = ThresholdPolicy(blackjack_dice).get_action\n",
        "# Let us check the average performance of the threshold policy on 100 blackjack with dice games.\n",
        "policy_thresh_eval = []\n",
        "for episodes in range(100):\n",
        "    policy_thresh_eval.append(blackjack_dice.one_episode(pol_thresh))\n",
        "\n",
        "\n",
        "print(\"mean +- std of the score of the uniform policy: {} +/- {}\".format(np.mean(policy_unif_eval), np.std(policy_unif_eval)))\n",
        "print(\"mean +- std of the score of the threshold policy: {} +/- {}\".format(np.mean(policy_thresh_eval), np.std(policy_thresh_eval)))\n"
      ],
      "metadata": {
        "id": "hHQtIZcXS0jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Dynamic Programming.\n",
        "## 2.1 State value function $V^{\\pi}(s)$\n",
        "One way to find the optimal policy for a MDP is to find the states with highest value, then $\\pi^*$ is the policy that visits those states. The value of a state, given a policy $\\pi$, is the expected cumulative reward obtained by $\\pi$ over MDP episodes when starting the episodes in $s_0 = s$. It is defined as follows with the Bellman expectation equation:\\\n",
        "$$V^{\\pi}(s) = \\underset{s'\\in S}{\\sum} P(s'|s, \\pi(s))(R(s, \\pi(s), s') + \\gamma V^{\\pi}(s'))$$\\\n",
        "The value of state $s$ is simply the reward obtained by $\\pi$ in state $s$, plus the the value of the expected next state $s'$.\n",
        "## 2.2 Exercise 3: Implement the Value Iteration algorithm\n",
        "The Bellman optimality equation defines an algorithm to find the value function associated with the optimal policy. \\\n",
        "$$V^*(s) = \\underset{a}{\\operatorname{max}}\\left [ \\underset{s'\\in S}{\\sum} P(s'|s, a)(R(s, a, s') + \\gamma V^{*}(s'))\\right]$$\n",
        "\n",
        "[The Value Iteration algorithm](http://incompleteideas.net/book/ebook/node44.html) \\\n",
        "[Notes on Value Iteration](http://chercheurs.lille.inria.fr/~lazaric/Webpage/MVA-RL_Course15_files/slides-lecture-02.pdf)"
      ],
      "metadata": {
        "id": "VB6bjWDR06fq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def VI(mdp, eps = 1e-2): #Value Iteration using the state value V\n",
        "\n",
        "    V = np.zeros((mdp.nb_states)) #initial state values are set to 0\n",
        "    list_error = []\n",
        "    quitt = False\n",
        "\n",
        "    while quitt==False:\n",
        "        Vold = V.copy()\n",
        "        for s in mdp.states: #for each state s\n",
        "            if s in mdp.terminal_states:\n",
        "                    continue\n",
        "            else:\n",
        "                # TODO\n",
        "                V[s] = ...\n",
        "\n",
        "        # Test if convergence has been reached\n",
        "        error = np.linalg.norm(V-Vold)\n",
        "        list_error.append(error)\n",
        "        if error < eps :\n",
        "            quitt = True\n",
        "\n",
        "    return V, list_error\n"
      ],
      "metadata": {
        "id": "UWIUK0Z-NN0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run VI on the blackjack with dice MDP. Plot the convergence of the algorithm for different thresholds $\\epsilon$ and different discount factors $\\gamma$. Observe the number of iterations required for convergence and compare with theory. \\\n",
        "Number of iterations: $$K=\\frac{\\operatorname{log}(r_{max}/\\epsilon)}{\\operatorname{log}(1/ \\gamma)}$$"
      ],
      "metadata": {
        "id": "p6-gLcQEftii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "V, list_error = VI(blackjack_dice, eps = 0.01)\n",
        "# TODO"
      ],
      "metadata": {
        "id": "9hARYJBXgBuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can visualize the optimal Value function:"
      ],
      "metadata": {
        "id": "F8G1lpgY92ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = V.reshape(-1,1)\n",
        "plt.imshow(tmp.T)\n",
        "plt.xlabel(\"$s$\")\n",
        "plt.ylabel(\"$V^*(s)$\")\n",
        "plt.yticks([])"
      ],
      "metadata": {
        "id": "VMgYba5f97NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Exercise 4: Get the optimal policy $\\pi^*$ from $V^*$.\n",
        "Implement the class PolicyFromV. It takes as input an MDP and the associated $V^*$. In state $s$, $\\pi^*(s)$ should return the action $a$ that leads to the next state $s'$ with highest value $V^*(s')$.\\\n",
        "$$\\pi^*(s) = \\underset{a}{\\operatorname{argmax}}\\left [ \\underset{s'\\in S}{\\sum} P(s'|s, a)(R(s, a, s') + \\gamma V^{*}(s'))\\right]$$"
      ],
      "metadata": {
        "id": "4kJ--Hc5WEpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyFromV(Policy):\n",
        "    def __init__(self, mdp, V):\n",
        "        super().__init__(mdp)\n",
        "        self.V = V\n",
        "        self.get_policy()\n",
        "\n",
        "    def get_policy(self):\n",
        "        self.policy = np.zeros(self.mdp.nb_states)\n",
        "        for s in self.mdp.states:\n",
        "            self.policy[s] = ... # TODO\n",
        "\n",
        "    def get_action(self, state):\n",
        "        return self.policy[state]"
      ],
      "metadata": {
        "id": "Ah81UT3fUFOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the performances of $\\pi^*$ with $\\pi^{\\text{unif}}$ and $\\pi^{\\text{thresh}}$ on 100 blackjack with dice games.  "
      ],
      "metadata": {
        "id": "ZM_4gpWheeWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "oh8XbJysevc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpret your policy. Can you explain how your policy $\\pi^*$ plays blackjack with dice ?"
      ],
      "metadata": {
        "id": "pNhciod9hR17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 State-action values $Q^{\\pi}(s,a)$\n",
        "Similarly to $V^{\\pi}(s)$ it is possible to define the state-aciton value of $s,a$ when following policy $\\pi$. The state-action value $Q^{\\pi}(s,a)$ is the expected cumulative reward obtained by $\\pi$ over MDP episodes when starting the episodes in $s_0 = s$ and choosing $a_0 = a$. \\\n",
        "$$Q^{\\pi}(s,a) = \\underset{s'\\in S}{\\sum} P(s'|s, a)(R(s, a, s') + \\gamma V^{\\pi}(s'))$$\\\n",
        "We also have: \\\n",
        "$$Q^*(s,a) = \\underset{s'\\in S}{\\sum} P(s'|s, a)(R(s, a, s') + \\gamma\\underset{a'}{\\operatorname{max}}Q^*(s',a'))$$"
      ],
      "metadata": {
        "id": "p7Sr83tLimBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Reinforcement Learning.\n",
        "By contrast with dynamic programming, a reinforcement learning agent is used when the MDP is unknown. More precisely, the state and action are known, but the agent does not know the transition nor the reward function.\n",
        "## 3.1 Q-Learning\n",
        "The Q-learning algorithm learns an estimate $\\hat{Q}$ of $Q^*$. It does so without an MDP transition probability model: it is a model-free RL algorithms. Q-learning collects transitions $(s,a,r,s')$ following a behaviour policy $\\pi^B$. For each collected transitions, $\\hat{Q}$ is updated as follows:\\\n",
        "$$\\hat{Q}(s,a) \\leftarrow \\hat{Q}(s,a) + \\alpha \\delta$$\\\n",
        "Where $\\delta$ is the temporal difference error:\\\n",
        "$$r + \\gamma \\underset{a'}{\\operatorname{max}}\\hat{Q}(s',a') - \\hat{Q}(s,a)$$\\\n",
        "The behaviour policy trades-off exploration and exploitation when collecting transitions in the MDP. For example, $\\pi^B$ can be the epsilon-greedy policy:\\\n",
        "$$\\pi^\\epsilon(s) = \\pi^{\\text{unif}}, \\text{ if } \\epsilon < 0.3$$\\\n",
        "$$\\pi^\\epsilon(s) = \\underset{a}{\\operatorname{argmax}}\\hat{Q}(s, a), \\text{ else }$$ \\\n",
        "A key aspect of an RL algorithm is how transitions are generated. If they are generated using the learned policy (or learned value function) or are they generated using a different policy ? In Q-Learning the transitions are generated using the behaviour policy that is different from the learned policy (the greedy policy with respect to $\\hat{Q}$): Q-Learning is an off-policy algorithm. Otherwise the RL algorithm is said to be on-policy (for example the [SARSA](http://incompleteideas.net/book/ebook/node64.html) algorithm).\n",
        "## 3.2 Exercise 5: Implement the Q-Learning algorithm\n",
        "[Q-Learning algorithm](http://incompleteideas.net/book/ebook/node65.html) \\\n",
        "[Notes on Q-Learning](http://chercheurs.lille.inria.fr/~lazaric/Webpage/MVA-RL_Course15_files/slides-lecture-03.pdf)"
      ],
      "metadata": {
        "id": "mUJkdPRFfWgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def QLearning(mdp, behaviour_pol, alpha=0.1, nb_iter=int(1e6)):\n",
        "    Qhat = np.zeros((mdp.nb_states, mdp.nb_actions))\n",
        "    list_Qs = []\n",
        "    s = mdp.reset()\n",
        "    for _ in range(nb_iter):\n",
        "        a = behaviour_pol(s, Qhat)\n",
        "        s_next, r, done = mdp.step(a)\n",
        "\n",
        "        if done:\n",
        "            Qhat[s, a] = r\n",
        "            s = mdp.reset()\n",
        "        else:\n",
        "            delta = ...# TODO\n",
        "            Qhat[s, a] = ...# TODO\n",
        "            list_Qs.append(Qhat.copy())\n",
        "            s = s_next\n",
        "\n",
        "    return Qhat, list_Qs\n",
        "\n",
        "class EpsGreedyPolicy(Policy):\n",
        "    def __init__(self, mdp, epsilon):\n",
        "        self.mdp = mdp\n",
        "        self.eps = epsilon\n",
        "\n",
        "    def get_action(self, state, Q):\n",
        "        if np.random.random() < self.eps:\n",
        "            return np.random.randint(self.mdp.nb_actions)\n",
        "        else:\n",
        "            return np.argmax(Q[state, :])"
      ],
      "metadata": {
        "id": "ZnjXuTUw89oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try Q-Learning to learn an estimate of the optimal state-action value function of the blackjack with dice MDP.\n",
        "- Visualize $\\hat{Q}$ returned by Q-Learning (use $\\texttt{plt.imshow()}$). Does this Q function makes sense ?\n",
        "- Make a plot to compare the convergence of $\\hat{Q}$ over iterations for different values of $\\epsilon$ and different learning rates $\\alpha$.\n",
        "- Compare the performance of the greedy policy $\\pi^{\\text{greedy}}(s) = \\underset{a}{\\operatorname{argmax}}\\hat{Q}(s, a)$ with other policies (uniform, threshold, $\\pi^*$) on 100 episodes."
      ],
      "metadata": {
        "id": "Fyd5P8UfCZ3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pol_behaviour = EpsGreedyPolicy(blackjack_dice, epsilon=0.3).get_action\n",
        "q, list_errors = QLearning(blackjack_dice, pol_behaviour)\n",
        "# TODO"
      ],
      "metadata": {
        "id": "ANQxsvF3ELTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Homework: Maze MDPs and Reinforcement Learning.\n",
        "This homework is open to interpretation and implementation.\n",
        "\n",
        "## Deadline 17/01/2024 23:59 email to hector.kohler@inria.fr and cc debabrota.basu@inria.fr\n",
        "\n",
        "- Exercise 1: implement a Maze MDP generator. Mazes are often use to benchmark new RL algorithms. One can think of a maze MDP as rectangle divided in cells. Each cells is a MDP state. The upper left cell is the initial state and the bottom right is the exit. Walls should be placed at random in the MDP: this will be done by specifying a fraction of cell states that are non-accessible. There are 4 actions in a maze MDP: left, right, up, dowm. All state-action pairs give a 0 reward, except a state-pair that leads to the exit (terminal) state in the bottom right cell.\n",
        "    - How to make sure that an agent can exit generated maze? You can try checking $V^*(s_{start}) > 0$.\n",
        "    - Your Maze MDP generator should take as arguments: a width, a height, and a fraction of cell walls. |S| = witdh x height.\n",
        "- Exercise 2: in a 3-pages report, present the performances of Q-Learning on maze MDPs.\n",
        "    - Compare eps-greedy policy and softmax behaviour policy.\n",
        "    - Compare different values of the exploration parameters (epsilon for eps-greedy and tau for softmax).\n",
        "    - Compare different learning rates.\n",
        "    - Compare different sizes and difficluties of Maze MDPs.\n",
        "    - Check that your learned Q functions make sense.\n",
        "- In general, describe your experiments and do not put too much figures (choose a few convincing figures) and then describe what you observe and try to understand if it makes sense.\n",
        "\n",
        "- Bonus: you can try to add a stochasticity parameter to your Maze MDP generator. One way to add stochasticity to the MDP could be to add a probability of action failure. For example, at every time step, the agent takes action $a$ but there is a probability that this action is not executed: \\\n",
        "```\n",
        "mdp = MDP(..., stochasticity = 0.1)\n",
        "...\n",
        "def step(self, action):\n",
        "    if np.random.random() < self.stoch:\n",
        "        action = None\n",
        "        return s, r, done\n",
        "```"
      ],
      "metadata": {
        "id": "FIL-xYROmRhm"
      }
    }
  ]
}